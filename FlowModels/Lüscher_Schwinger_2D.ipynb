{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuciusBushnaq/PhDQCD/blob/main/FlowModels/L%C3%BCscher_Schwinger_2D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjPf577b7nuB"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import io\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import cmath\n",
        "print(f'TORCH VERSION: {torch.__version__}') \n",
        "import packaging.version\n",
        "if packaging.version.parse(torch.__version__) < packaging.version.parse('1.5.0'): \n",
        "    raise RuntimeError('Torch versions lower than 1.5.0 not supported')\n",
        "    \n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "pd.options.display.max_seq_items = 20000\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch_device = 'cuda'\n",
        "    float_dtype = np.float32 # single \n",
        "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "else:\n",
        "    torch_device = 'cpu'\n",
        "    float_dtype = np.float64 # double \n",
        "    torch.set_default_tensor_type(torch.DoubleTensor)\n",
        "print(f\"TORCH DEVICE: {torch_device}\")\n",
        "\n",
        "#Useful general functions\n",
        "def torch_mod(x):\n",
        "    return torch.remainder(x, 2*np.pi)\n",
        "def torch_wrap(x):\n",
        "    return torch_mod(x+np.pi) - np.pi\n",
        "def grab(var):\n",
        "    return var.detach().cpu().numpy()\n",
        "#Useful torch.roll for field with AP boundary conditions in time direction\n",
        "def roll_AP(x,s,d):\n",
        "    if (d==2):\n",
        "        assert s==1 or s==-1, 'roll_AP can only handle shifts by one lattice site' \n",
        "        x_s=torch.roll(x, s, d)\n",
        "        if (s==1):\n",
        "            x_s[:,:,0]=-x_s[:,:,0]\n",
        "        if (s==-1):\n",
        "            x_s[:,:,-1]=-x_s[:,:,-1]\n",
        "        return x_s\n",
        "    else:\n",
        "        return torch.roll(x, s, d)\n",
        "\n",
        "def roll_AP_pt(x,s,d):\n",
        "    if (d==3):\n",
        "        assert s==1 or s==-1, 'roll_AP can only handle shifts by one lattice site' \n",
        "        x_s=torch.roll(x, s, d)\n",
        "        if (s==1):\n",
        "            x_s[:,:,:,0]=-x_s[:,:,:,0]\n",
        "        if (s==-1):\n",
        "            x_s[:,:,:,-1]=-x_s[:,:,:,-1]\n",
        "        return x_s\n",
        "    else:\n",
        "        return torch.roll(x, s, d)\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "#Stuff for pretty plots\n",
        "from IPython.display import display\n",
        "def init_live_plot(dpi=125, figsize=(8,4)):\n",
        "    fig, ax_ess = plt.subplots(1,1, dpi=dpi, figsize=figsize)\n",
        "    plt.xlim(0, N_era*N_epoch)\n",
        "    plt.ylim(0, 0.4)\n",
        "    ess_line = plt.plot([0],[0], alpha=0.5) # dummy plt.grid(False)\n",
        "    plt.ylabel('ESS')\n",
        "    ax_loss = ax_ess.twinx()\n",
        "    loss_line = plt.plot([0],[0], alpha=0.5, c='orange') # dummy plt.grid(False)\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    display_id = display(fig, display_id=True)\n",
        "    return dict(\n",
        "        fig=fig, ax_ess=ax_ess, ax_loss=ax_loss,\n",
        "        ess_line=ess_line, loss_line=loss_line,\n",
        "        display_id=display_id\n",
        "    )\n",
        "def moving_average(x, window=10):\n",
        "    if len(x) < window:\n",
        "        return np.mean(x, keepdims=True)\n",
        "    else:\n",
        "        return np.convolve(x, np.ones(window), 'valid') / window\n",
        "def update_plots(history, fig, ax_ess, ax_loss, ess_line, loss_line, display_id): \n",
        "    Y = np.array(history['ess'])\n",
        "    Y = moving_average(Y, window=15)\n",
        "    ess_line[0].set_ydata(Y)\n",
        "    ess_line[0].set_xdata(np.arange(len(Y))) \n",
        "    Y = history['loss']\n",
        "    Y = moving_average(Y, window=15) \n",
        "    loss_line[0].set_ydata(np.array(Y)) \n",
        "    loss_line[0].set_xdata(np.arange(len(Y))) \n",
        "    ax_loss.relim()\n",
        "    ax_loss.autoscale_view()\n",
        "    fig.canvas.draw()\n",
        "    display_id.update(fig) # need to force colab to update plot\n",
        "\n",
        "# init weights in a way that gives interesting behavior without training\n",
        "def set_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight is not None:\n",
        "        torch.nn.init.normal_(m.weight, mean=1, std=2) \n",
        "    if hasattr(m, 'bias') and m.bias is not None:\n",
        "        m.bias.data.fill_(-1)    \n",
        "# init weights\n",
        "def set_weights_xavier(m):\n",
        "    if hasattr(m, 'weight') and m.weight is not None:\n",
        "        torch.nn.init.xavier_normal_(m.weight, gain=0.5) \n",
        "    if hasattr(m, 'bias') and m.bias is not None:\n",
        "        torch.nn.init.xavier_normal_(m.bias, gain=0.5) \n",
        "def set_weights_kaiming(m):\n",
        "    if hasattr(m, 'weight') and m.weight is not None:\n",
        "        torch.nn.init.kaiming_uniform_(m.weight, gain=nn.init.calculate_gain('leaky_relu')) \n",
        "    if hasattr(m, 'bias') and m.bias is not None:\n",
        "        torch.nn.init.kaiming_uniform_(m.bias, gain=nn.init.calculate_gain('leaky_relu')) \n",
        "\n",
        "    \n",
        "#General stuff for training    \n",
        "def calc_dkl(logp, logq):\n",
        "    return (logq - logp).mean() # reverse KL, assuming samples from q\n",
        "def apply_flow_to_prior(prior_U, prior_x, coupling_layers, *, batch_size):\n",
        "    U = prior_U.sample_n(batch_size)\n",
        "    x = prior_x.sample_n(batch_size)\n",
        "    logq_U = prior_U.log_prob(U)\n",
        "    logq_x = prior_x.log_prob(x)\n",
        "    assert not torch.isnan(logq_U).any(), f\"NaN in the prior logq_U={logq_U[0]}\"\n",
        "    assert not torch.isinf(logq_x).any(), f\"inf in the prior logq_x={logq_x[0]}\"\n",
        "    #print(f'log_x {logq_x[0]}')\n",
        "    #print(f'log_U {logq_U[0]}')\n",
        "    logq=logq_U+logq_x\n",
        "    assert not torch.isnan(logq).any(), f\"NaN in the prior logq={logq[0]}\"\n",
        "    #logq=logq_U\n",
        "    i=0\n",
        "    for layer in coupling_layers:\n",
        "        U, x, logJ = layer.forward(U, x)\n",
        "\n",
        "        logq = logq - logJ\n",
        "        #print((logJ[0],i))\n",
        "        i=i+1\n",
        "    return U, x, logq\n",
        "def train_step(model, action, loss_fn, optimizer, metrics): \n",
        "    layers, prior_U, prior_x = model['layers'], model['prior_U'], model['prior_x']\n",
        "    optimizer.zero_grad()\n",
        "    U, x, logq = apply_flow_to_prior(prior_U, prior_x, layers, batch_size=batch_size)\n",
        "    logp, E = action(U,x)\n",
        "    logp=-logp\n",
        "    E=-E\n",
        "    #assert not torch.isinf(logp).any(), f\"inf in the logp={logp[0]}\"\n",
        "    #assert not torch.isinf(logq).any(), f\"inf in the logq={logq[0]}\"\n",
        "    loss = calc_dkl(logp, logq)\n",
        "    #assert not torch.isinf(loss).any(), f\"inf in the loss={loss[0]}\"\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    metrics['loss'].append(grab(loss)) \n",
        "    metrics['logp'].append(grab(logp)) \n",
        "    metrics['logq'].append(grab(logq))\n",
        "    metrics['ess'].append(grab( compute_ess(logp, logq) ))\n",
        "    metrics['E'].append(grab(E))\n",
        "    metrics['x'].append(grab(x[0]))\n",
        "def compute_ess(logp, logq):\n",
        "    logw = logp - logq\n",
        "    log_ess = 2*torch.logsumexp(logw, dim=0) - torch.logsumexp(2*logw, dim=0)\n",
        "    ess_per_cfg = torch.exp(log_ess) / len(logw)\n",
        "    return ess_per_cfg\n",
        "def print_metrics(history, avg_last_N_epochs): \n",
        "    print(f'== Era {era} | Epoch {epoch} metrics ==') \n",
        "    for key, val in history.items():\n",
        "        avgd = np.mean(val[-avg_last_N_epochs:]) \n",
        "        print(f'\\t{key} {avgd:g}')     \n",
        "#Our prior\n",
        "class Fermion_nf(torch.nn.Module): \n",
        "  \"\"\"Gaussian normal draw samples\"\"\" \n",
        "  def __init__(self, a, b, c, d):\n",
        "    super().__init__()\n",
        "    self.distreal= torch.distributions.normal.Normal(a, b)\n",
        "    self.distimag= torch.distributions.normal.Normal(c, d)\n",
        "  def log_prob(self, x):\n",
        "    axes = range(1, len(x.shape))\n",
        "    return torch.sum( self.distreal.log_prob(x.real)+self.distimag.log_prob(x.imag), dim=tuple(axes) ) \n",
        "  def sample_n(self, batch_size):\n",
        "    real=self.distreal.sample((batch_size,))\n",
        "    imag=self.distimag.sample((batch_size,))\n",
        "    return torch.complex(real,imag)\n",
        "\n",
        "class MultivariateUniform(torch.nn.Module): \n",
        "    \"\"\"Uniformly draw samples from [a,b]\"\"\" \n",
        "    def __init__(self, a, b):\n",
        "        super().__init__()\n",
        "        self.dist = torch.distributions.uniform.Uniform(a, b)\n",
        "    def log_prob(self, x):\n",
        "        axes = range(1, len(x.shape))\n",
        "        return torch.sum(self.dist.log_prob(x), dim=tuple(axes))\n",
        "    def sample_n(self, batch_size):\n",
        "        return self.dist.sample((batch_size,))\n",
        "    \n",
        "#Stuff for calculating U(1) theory\n",
        "def compute_u1_plaq(links, mu, nu):\n",
        "    \"\"\"Compute U(1) plaquettes in the (mu,nu) plane given `links` = arg(U)\"\"\" \n",
        "    return (links[:,mu] + torch.roll(links[:,nu], -1, mu+1)\n",
        "            - torch.roll(links[:,mu], -1, nu+1) - links[:,nu])\n",
        "class TestAction:\n",
        "  def __init__(self, prior_U,prior_x):\n",
        "    self.prior_U=prior_U\n",
        "    self.prior_x=prior_x\n",
        "  def __call__(self, U,x):\n",
        "    gauge_action_density=-prior_U.log_prob(U)\n",
        "    Fermion_action_density=-prior_x.log_prob(x)\n",
        "    return gauge_action_density+Fermion_action_density, gauge_action_density\n",
        "\n",
        "\n",
        "class U1GaugeAction:\n",
        "    def __init__(self, beta):\n",
        "        self.beta = beta\n",
        "    def __call__(self, cfgs):\n",
        "        Nd = cfgs.shape[1]\n",
        "        action_density = 0\n",
        "        for mu in range(Nd):\n",
        "            for nu in range(mu+1,Nd):\n",
        "                action_density = action_density + torch.cos(\n",
        "                    compute_u1_plaq(cfgs, mu, nu))\n",
        "        return -self.beta * torch.sum(action_density, dim=tuple(range(1,Nd+1)))\n",
        "    \n",
        "#Stuff for calculating U(1) interaction  \n",
        "def pauli(mu,field):\n",
        "    pauli=[0,0,0]\n",
        "    pauli[0]=torch.complex(torch.tensor([[0.0,1.0],[1.0,0.0]]),torch.zeros(2,2))\n",
        "    pauli[1]=torch.complex(torch.zeros(2,2),torch.tensor([[0.0,-1.0],[1.0,0.0]]))\n",
        "    pauli[2]=torch.complex(torch.tensor([[1.0,0.0],[0.0,-1.0]]),torch.zeros(2,2))\n",
        "\n",
        "    field=field.unsqueeze(-1)\n",
        "    return np.sign(mu)*torch.matmul(pauli[abs(mu)],field).squeeze(-1)\n",
        "def spinstruct(mu,x):\n",
        "    if mu==2:\n",
        "        return pauli(mu,x)\n",
        "    else: \n",
        "        return pauli(2,x-pauli(mu,x))\n",
        "\n",
        "    \n",
        "def compute_Q(links,x,m):\n",
        "        \"\"\"Compute gamma_5 (pauli_3) times the normalised Dirac operator for the input\"\"\" \n",
        "        result= ((m+2.0)/(m+4.0))*spinstruct(2,x)\n",
        "        for mu in range(0,links.shape[1]):\n",
        "            forw=torch.unsqueeze(links[:,mu],3)*spinstruct(mu,roll_AP(x,-1,mu+1))\n",
        "            back=torch.unsqueeze(torch.conj(torch.roll(links[:,mu], +1, mu+1)),3)*spinstruct(-mu,roll_AP(x,+1,mu+1))\n",
        "            result+=-0.5*(forw+back)/(m+4.0)\n",
        "        #assert not torch.isnan(result).any(), f\"NaN in the compute_d={result[0]}\"\n",
        "        #assert not torch.isinf(result).any(), f\"inf in the compute_d={result[0]}\"\n",
        "        return result\n",
        "\n",
        "\n",
        "def compute_parallel_transport(x,links, parity):\n",
        "  temp=[]\n",
        "  temp.append(torch.select(x,4,parity))\n",
        "  for mu in range(0,links.shape[1]):\n",
        "    for s in range(0,x.shape[4]):\n",
        "      temp.append(torch.unsqueeze(links[:,mu],1)*roll_AP_pt(torch.select(x,4,s),-1,mu+2))\n",
        "      temp.append(torch.unsqueeze(torch.conj(torch.roll(links[:,mu], +1, mu+1)),1)*roll_AP_pt(torch.select(x,4,s),+1,mu+2))\n",
        "  return torch.stack(temp,dim=4)\n",
        "\n",
        "class TotalAction(U1GaugeAction):\n",
        "    def __init__(self, beta, S_f,n_f):\n",
        "        super().__init__(beta)\n",
        "        self.S_f=S_f\n",
        "    def __call__(self, U, x):\n",
        "        Nd = U.shape[1]\n",
        "        gauge_action_density = super().__call__(U)\n",
        "        #assert not torch.isinf(gauge_action_density).any(), f\"inf in the gauge_action={gauge_action_density[0]}\"\n",
        "        links=torch.polar(torch.ones(U.shape),U)\n",
        "        Fermion_action=0\n",
        "        for k in range(0,n_f):\n",
        "          Fermion_action+=(compute_Q(links,x[:,k],self.S_f[k][2])-self.S_f[k][0]*x[:,k]).abs()**2+self.S_f[k][1]*x[:,k].abs()**2\n",
        "        Fermion_action_density=torch.sum(Fermion_action, dim=tuple(range(1,Nd+2)))\n",
        "        #assert not torch.isinf(Fermion_action_density).any(), f\"inf in the gauge_action={Fermion_action_density[0]}\"\n",
        "        return gauge_action_density+Fermion_action_density, gauge_action_density\n",
        "\n",
        "#Our convnet setup function\n",
        "def make_conv_net(*, hidden_sizes, dilation_sizes, kernel_size, in_channels, out_channels, use_final_tanh): \n",
        "    sizes = [in_channels] + hidden_sizes + [out_channels]\n",
        "    assert packaging.version.parse(torch.__version__) >= packaging.version.parse('1.5.0') \n",
        "    assert kernel_size % 2 == 1, 'kernel size must be odd for PyTorch >= 1.5.0' \n",
        "    net = []\n",
        "    for i in range(len(sizes) - 1):\n",
        "      padding_size = (  (kernel_size+(kernel_size-1)*(dilation_sizes[i]-1)) // 2)\n",
        "      net.append(torch.nn.Conv2d(\n",
        "          sizes[i], sizes[i+1], kernel_size, padding=padding_size, dilation=dilation_sizes[i], stride=1, padding_mode='circular'))\n",
        "      if i != len(sizes) - 2:\n",
        "            net.append(torch.nn.ELU())\n",
        "      else:\n",
        "          if use_final_tanh:\n",
        "              net.append(torch.nn.Tanh())\n",
        "    return torch.nn.Sequential(*net)\n",
        "\n",
        "\n",
        "#Our gauge invariant affine coupling layer for U(1)\n",
        "class GaugeEquivCouplingLayer(torch.nn.Module):\n",
        "    \"\"\"U(1) gauge equiv coupling layer defined by `plaq_coupling` acting on plaquettes.\"\"\" \n",
        "    def __init__(self, *, lattice_shape, mask_mu, mask_off, plaq_coupling):\n",
        "        super().__init__()\n",
        "        link_mask_shape = (len(lattice_shape),) + lattice_shape\n",
        "        self.active_mask = make_2d_link_active_stripes(link_mask_shape, mask_mu, mask_off)\n",
        "        self.plaq_coupling = plaq_coupling\n",
        "    def forward(self, U,x):\n",
        "        plaq = compute_u1_plaq(U, mu=0, nu=1)\n",
        "        new_plaq, logJ = self.plaq_coupling(plaq)\n",
        "        delta_plaq = new_plaq - plaq\n",
        "        delta_links = torch.stack((delta_plaq, -delta_plaq), dim=1) # signs for U vs Udagger \n",
        "        fU = self.active_mask * torch_wrap(delta_links + U) + (1-self.active_mask) * U\n",
        "        assert not torch.isnan(logJ).any(), f\"NaN in the gauge logJ={logJ[0]}\"\n",
        "        assert not torch.isinf(logJ).any(), f\"inf in the gauge logJ={logJ[0]}\"\n",
        "        return fU, x, logJ\n",
        "    def reverse(self, fU,x):\n",
        "        new_plaq = compute_u1_plaq(x, mu=0, nu=1)\n",
        "        plaq, logJ = self.plaq_coupling.reverse(new_plaq)\n",
        "        delta_plaq = plaq - new_plaq\n",
        "        delta_links = torch.stack((delta_plaq, -delta_plaq), dim=1) # signs for U vs Udagger \n",
        "        U = self.active_mask * torch_wrap(delta_links + fU) + (1-self.active_mask) * fU \n",
        "        return U, x, logJ\n",
        "\n",
        "\n",
        "class FermionCouplingLayer(torch.nn.Module):\n",
        "    def __init__(self, *, x_shape, parity_l, parity_s, A_coupling,B_coupling,k,action):\n",
        "        super().__init__()\n",
        "        self.parity_s=parity_s\n",
        "        self.active_mask = make_checker_mask_f(x_shape, parity_l,parity_s, k=k)\n",
        "        self.logJ_mask = make_checker_mask((x_shape[1],x_shape[2]), parity=parity_l)\n",
        "        self.A_coupling = A_coupling\n",
        "        self.B_coupling = B_coupling\n",
        "        self.k=k\n",
        "        self.a=action[0]*torch.ones((x_shape[1],x_shape[1]))\n",
        "        self.b=action[1]*torch.ones((x_shape[1],x_shape[1]))\n",
        "    def forward(self, U, x):\n",
        "        plaq = compute_u1_plaq(U, mu=0, nu=1)\n",
        "        links = torch.polar(torch.ones(U.shape),U)\n",
        "\n",
        "        T = compute_parallel_transport(x,links,1-self.parity_s)\n",
        "        A= self.A_coupling(stack_f(plaq,self.a,self.b))\n",
        "        B= self.B_coupling(stack_f(plaq,self.a,self.b))\n",
        "\n",
        "        A_c=torch.exp(torch.complex(A[:,0],A[:,1]))\n",
        "        #test\n",
        "        #A_c=torch.complex(torch.ones(A_c.shape, dtype=torch.float),torch.ones(A_c.shape, dtype=torch.float))\n",
        "        B_c=torch.complex(B[:,0:9],B[:,9:18])\n",
        "        B_c=B_c.permute(0,2,3,1)\n",
        "        #B_c=B_c.reshape(B.size(0),x_shape[1],x_shape[2],x_shape[3],4*x_shape[3])\n",
        "        #test\n",
        "        #B_c=torch.complex(0.1*torch.ones(B_c.shape, dtype=torch.float),0.1*torch.ones(B_c.shape, dtype=torch.float))\n",
        "        #fx_active= self.active_mask * torch.matmul(A_c.unsqueeze(1),x.unsqueeze(-1)).squeeze(-1)\n",
        "        fx_active= self.active_mask * ((A_c.unsqueeze(1)).unsqueeze(-1))*x\n",
        "        fT=self.active_mask*(torch.matmul((B_c.unsqueeze(1)).unsqueeze(-2),T.unsqueeze(-1)).squeeze(-1))\n",
        "        #fT=self.active_mask*B_c.unsqueeze(1)*T\n",
        "        fx=(1-self.active_mask)*x+fx_active+fT\n",
        "        local_logJ = 2.0*self.logJ_mask*torch.log(torch.abs(A_c))\n",
        "        axes = tuple(range(1, len(local_logJ.shape)))\n",
        "        logJ = torch.sum(local_logJ, dim=axes)\n",
        "        assert not torch.isnan(logJ).any(), f\"NaN in the fermion logJ={logJ[0]}\"\n",
        "        assert not torch.isinf(logJ).any(), f\"inf in the fermion logJ={logJ[0]}\"\n",
        "        return U, fx, logJ\n",
        "    \n",
        "\n",
        "    \n",
        "#Masking patterns for the U(1) layers and a small test for them    \n",
        "def make_2d_link_active_stripes(shape, mu, off): \n",
        "    \"\"\"\n",
        "    Stripes mask looks like in the `mu` channel (mu-oriented links)::\n",
        "    10001000100 \n",
        "    10001000100 \n",
        "    10001000100 \n",
        "    10001000100\n",
        "    where vertical is the `mu` direction, and the pattern is offset in the nu\n",
        "    direction by `off` (mod 4). The other channel is identically 0.\n",
        "    \"\"\"\n",
        "    assert len(shape) == 2+1, 'need to pass shape suitable for 2D gauge theory' \n",
        "    assert shape[0] == len(shape[1:]), 'first dim of shape must be Nd'\n",
        "    assert mu in (0,1), 'mu must be 0 or 1'\n",
        "    mask = np.zeros(shape).astype(np.uint8)\n",
        "    if mu == 0:\n",
        "        mask[mu,:,0::4] = 1\n",
        "    elif mu == 1:\n",
        "        mask[mu,0::4] = 1\n",
        "    nu = 1-mu\n",
        "    mask = np.roll(mask, off, axis=nu+1)\n",
        "    return torch.from_numpy(mask.astype(float_dtype)).to(torch_device)\n",
        "\n",
        "def make_single_stripes(shape, mu, off): \n",
        "    \"\"\"\n",
        "    10001000100 \n",
        "    10001000100 \n",
        "    10001000100 \n",
        "    10001000100\n",
        "    where vertical is the `mu` direction. Vector of 1 is repeated every 4.\n",
        "    The pattern is offset in perpendicular to the mu direction by `off` (mod 4). \n",
        "    \"\"\"\n",
        "    assert len(shape) == 2, 'need to pass 2D shape'\n",
        "    assert mu in (0,1), 'mu must be 0 or 1'\n",
        "    mask = np.zeros(shape).astype(np.uint8)\n",
        "    if mu == 0:\n",
        "        mask[:,0::4] = 1\n",
        "    elif mu == 1:\n",
        "        mask[0::4] = 1\n",
        "    mask = np.roll(mask, off, axis=1-mu)\n",
        "    return torch.from_numpy(mask).to(torch_device)\n",
        "def make_double_stripes(shape, mu, off): \n",
        "    \"\"\"\n",
        "    Double stripes mask looks like::\n",
        "    11001100 \n",
        "    11001100 \n",
        "    11001100 \n",
        "    11001100\n",
        "    where vertical is the `mu` direction. The pattern is offset in perpendicular to the mu direction by `off` (mod 4).\n",
        "    \"\"\"\n",
        "    assert len(shape) == 2, 'need to pass 2D shape'\n",
        "    assert mu in (0,1), 'mu must be 0 or 1'\n",
        "    mask = np.zeros(shape).astype(np.uint8)\n",
        "    if mu == 0:\n",
        "        mask[:,0::4] = 1\n",
        "        mask[:,1::4] = 1\n",
        "    elif mu == 1:\n",
        "        mask[0::4] = 1\n",
        "        mask[1::4] = 1\n",
        "    mask = np.roll(mask, off, axis=1-mu)\n",
        "    return torch.from_numpy(mask).to(torch_device)\n",
        "def make_plaq_masks(mask_shape, mask_mu, mask_off):\n",
        "    mask = {}\n",
        "    mask['frozen'] = make_double_stripes(mask_shape, mask_mu, mask_off+1) \n",
        "    mask['active'] = make_single_stripes(mask_shape, mask_mu, mask_off) \n",
        "    mask['passive'] = 1 - mask['frozen'] - mask['active']\n",
        "    return mask\n",
        "\n",
        "#Checkerboard Masking pattern for the fermions\n",
        "def make_checker_mask(shape, parity,):\n",
        "    checker = torch.ones(shape, dtype=torch.uint8)-parity\n",
        "    checker[::2, ::2] = parity\n",
        "    checker[1::2, 1::2] = parity\n",
        "    return checker.to(torch_device)\n",
        "\n",
        "def make_checker_mask_f(shape, parity_l,parity_s, k):\n",
        "    checker = torch.zeros(shape, dtype=torch.uint8)\n",
        "    checker[k,1::2, ::2,parity_s] = 1-parity_l\n",
        "    checker[k,::2, 1::2,parity_s] = 1-parity_l\n",
        "    checker[k,::2, ::2,parity_s] = parity_l\n",
        "    checker[k,1::2, 1::2,parity_s] = parity_l\n",
        "    return checker.to(torch_device)\n",
        "\n",
        "#Helper functions for the inner plaquette coupling layer\n",
        "def tan_transform(x, s):\n",
        "    #assert not torch.isnan(torch.exp(s)).any(), f\"NaN in the exp(s)={torch.exp(s)[0],s[0]}\"\n",
        "    #assert not torch.isinf(torch.exp(s)).any(), f\"inf in the exp(s)={torch.exp(s)[0],s[0]}\"\n",
        "    #assert not torch.isnan(2*torch.atan(torch.tan(x/2)*torch.exp(s))).any(), f\"NaN in the tan={2*torch.atan(torch.tan(x/2)*torch.exp(s))[0]}\"\n",
        "    #assert not torch.isinf(2*torch.atan(torch.tan(x/2)*torch.exp(s))).any(), f\"inf in the tan={2*torch.atan(torch.tan(x/2)*torch.exp(s))[0]}\"\n",
        "    return 2*torch.atan(torch.tan(x/2)*torch.exp(s))\n",
        "def tan_transform_logJ(x, s):\n",
        "    return -torch.log(torch.exp(-s)*torch.cos(x/2)**2 + torch.exp(s)*torch.sin(x/2)**2)\n",
        "def mixture_tan_transform(x, s):\n",
        "    assert len(x.shape) == len(s.shape), \\\n",
        "    f'Dimension mismatch between x and s {x.shape} vs {s.shape}' \n",
        "    mean=torch.mean(tan_transform(x, s), dim=1, keepdim=True)\n",
        "    #assert not torch.isnan(mean).any(), f\"NaN in the mean={mean[0]}\"\n",
        "    #assert not torch.isinf(mean).any(), f\"inf in the mean={mean[0]}\"\n",
        "    return mean\n",
        "def mixture_tan_transform_logJ(x, s):\n",
        "    assert len(x.shape) == len(s.shape), \\\n",
        "    f'Dimension mismatch between x and s {x.shape} vs {s.shape}'\n",
        "    return torch.logsumexp(tan_transform_logJ(x, s), dim=1) - np.log(s.shape[1])\n",
        "\n",
        "def invert_transform_bisect(y, *, f, tol, max_iter, a=(-1)*np.pi, b=np.pi):\n",
        "    min_x = a*torch.ones_like(y)\n",
        "    max_x = b*torch.ones_like(y)\n",
        "    min_val = f(min_x)\n",
        "    max_val = f(max_x)\n",
        "    with torch.no_grad():\n",
        "        for i in range(max_iter):\n",
        "            mid_x = (min_x + max_x) / 2\n",
        "            mid_val = f(mid_x)\n",
        "            greater_mask = (y > mid_val).int()\n",
        "            greater_mask = greater_mask.float()\n",
        "            err = torch.max(torch.abs(y - mid_val))\n",
        "            if err < tol: return mid_x\n",
        "            if torch.all((mid_x == min_x) + (mid_x == max_x)):\n",
        "                print('WARNING: Reached floating point precision before tolerance ' f'(iter {i}, err {err})')\n",
        "                return mid_x\n",
        "            min_x = greater_mask*mid_x + (1-greater_mask)*min_x\n",
        "            min_val = greater_mask*mid_val + (1-greater_mask)*min_val\n",
        "            max_x = (1-greater_mask)*mid_x + greater_mask*max_x\n",
        "            max_val = (1-greater_mask)*mid_val + greater_mask*max_val\n",
        "        print(f'WARNING: Did not converge to tol {tol} in {max_iter} iters! Error was {err}') \n",
        "        return mid_x\n",
        "    \n",
        "def stack_cos_sin(x):\n",
        "    return torch.stack((torch.cos(x), torch.sin(x)), dim=1)\n",
        "\n",
        "def stack_f(x,a,b):\n",
        "    a_x=(a.unsqueeze(0)).expand(x.size(0),-1,-1)\n",
        "    b_x=(b.unsqueeze(0)).expand(x.size(0),-1,-1)\n",
        "    return torch.stack((torch.cos(x), torch.sin(x),a_x,b_x), dim=1)\n",
        "\n",
        "#The inner plaquette coupling layer\n",
        "class NCPPlaqCouplingLayer(torch.nn.Module):\n",
        "    def __init__(self, net, *, mask_shape, mask_mu, mask_off,\n",
        "        inv_prec=1e-6, inv_max_iter=1000):\n",
        "        super().__init__()\n",
        "        assert len(mask_shape) == 2, (\n",
        "        f'NCPPlaqCouplingLayer is implemented only in 2D, ' \n",
        "        f'mask shape {mask_shape} is invalid')\n",
        "        self.mask = make_plaq_masks(mask_shape, mask_mu, mask_off)\n",
        "        self.net = net\n",
        "        self.inv_prec = inv_prec\n",
        "        self.inv_max_iter = inv_max_iter\n",
        "        \n",
        "    def forward(self, P):\n",
        "        P2 = self.mask['frozen'] * P\n",
        "        net_out = self.net(stack_cos_sin(P2))\n",
        "        assert net_out.shape[1] >= 2, 'CNN must output n_mix (s_i) + 1 (t) channels' \n",
        "        s, t = net_out[:,:-1], net_out[:,-1]\n",
        "        P1 = self.mask['active'] * P\n",
        "        P1 = P1.unsqueeze(1)\n",
        "        #assert not torch.isnan(s).any(), f\"NaN in the plaquette s={s[0]}\"\n",
        "        #assert not torch.isinf(s).any(), f\"inf in the plaquette s={s[0]}\"\n",
        "        local_logJ = self.mask['active'] * mixture_tan_transform_logJ(P1, s) \n",
        "        axes = tuple(range(1, len(local_logJ.shape)))\n",
        "        logJ = torch.sum(local_logJ, dim=axes)\n",
        "        fP1 = self.mask['active'] * mixture_tan_transform(P1, s).squeeze(1)\n",
        "        \n",
        "        fP = (\n",
        "            self.mask['active'] * (fP1 + t) + \n",
        "            self.mask['passive'] * P + \n",
        "            self.mask['frozen'] * P)\n",
        "        #assert not torch.isnan(P).any(), f\"NaN in the plaq={P[0]}\"\n",
        "        #assert not torch.isinf(P).any(), f\"inf in the plaq={P[0]}\"\n",
        "        #assert not torch.isnan(logJ).any(), f\"NaN in the logJ={logJ[0]}\"\n",
        "        #assert not torch.isinf(logJ).any(), f\"inf in the logJ={logJ[0]}\"\n",
        "        return fP, logJ\n",
        "    def reverse(self, fP):\n",
        "        fP2 = self.mask['frozen'] * fP\n",
        "        net_out = self.net(stack_cos_sin(fP2))\n",
        "        assert net_out.shape[1] >= 2, 'CNN must output n_mix (s_i) + 1 (t) channels' \n",
        "        s, t = net_out[:,:-1], net_out[:,-1]\n",
        "        P1 = self.mask['active'] * (fP - t).unsqueeze(1)\n",
        "        transform = lambda x: self.mask['active'] * mixture_tan_transform(x, s) \n",
        "        P1 = invert_transform_bisect(P1, f=transform, tol=self.inv_prec, max_iter=self.inv_max_iter) \n",
        "        local_logJ = self.mask['active'] * mixture_tan_transform_logJ(P1, s) \n",
        "        axes = tuple(range(1, len(local_logJ.shape)))\n",
        "        logJ = -torch.sum(local_logJ, dim=axes)\n",
        "        P1 = P1.squeeze(1)\n",
        "        P=(\n",
        "            self.mask['active'] * P1 + \n",
        "            self.mask['passive'] * fP + \n",
        "            self.mask['frozen'] * fP2)\n",
        "        return P, logJ\n",
        "    \n",
        "#Our actual function for the whole thing for the coupled U(1) and coupled fermion\n",
        "def make_phi4u1_equiv_layers(*, n_layers_g, n_layers_f, n_mixture_comps, lattice_shape, x_shape, hidden_sizes, dilation_sizes, kernel_size,n_f,f_S):\n",
        "    layers = []\n",
        "    \n",
        "    out_channels = n_mixture_comps + 1 # for mixture s and t, respectively \n",
        "    out_channels_A=2 # (spin)x(spin) complex number\n",
        "    out_channels_B=18 # (1)x(2*dimension*spin+1) complex matrix\n",
        "    for i in range(n_layers_g):\n",
        "        in_channels = 2 # P - > (cos(P), sin(P))\n",
        "        mu = i % 2\n",
        "        offU = (i//2) % 4\n",
        "        net_plaq = make_conv_net(in_channels=in_channels, out_channels=out_channels,hidden_sizes=hidden_sizes[0], dilation_sizes=dilation_sizes[0], kernel_size=kernel_size,use_final_tanh=False)\n",
        "        \n",
        "        plaq_coupling = NCPPlaqCouplingLayer(net_plaq, mask_shape=lattice_shape, mask_mu=mu, mask_off=offU)\n",
        "        link_coupling = GaugeEquivCouplingLayer(lattice_shape=lattice_shape, mask_mu=mu, mask_off=offU,plaq_coupling=plaq_coupling)\n",
        "        \n",
        "        layers.append(link_coupling)\n",
        "    for j in range(n_layers_f):\n",
        "      in_channels = 4 # P - > (cos(P), sin(P)), a, b\n",
        "      parity_s = j % 2\n",
        "      parity_l = (j//2) % 2\n",
        "      net_A=make_conv_net(in_channels=in_channels, out_channels=out_channels_A,hidden_sizes=hidden_sizes[1], dilation_sizes=dilation_sizes[1],kernel_size=kernel_size,use_final_tanh=False)\n",
        "      net_B=make_conv_net(in_channels=in_channels, out_channels=out_channels_B,hidden_sizes=hidden_sizes[2], dilation_sizes=dilation_sizes[2],kernel_size=kernel_size,use_final_tanh=False)\n",
        "      for k in range(n_f):\n",
        "        fermion_coupling = FermionCouplingLayer(x_shape=x_shape, parity_l=parity_l,parity_s=parity_s,A_coupling=net_A,B_coupling=net_B,k=k,action=f_S[k])\n",
        "        layers.append(fermion_coupling)\n",
        "\n",
        "    return torch.nn.ModuleList(layers)\n",
        "\n",
        "#polynomial root setup\n",
        "def naivepolygen(n_f,m_f):\n",
        "  f_S=[]\n",
        "  for i in range(1,n_f+1):\n",
        "    z=1.0-np.exp(2j*i*np.pi/(n_f+1.0))\n",
        "    sqz=cmath.sqrt(z)\n",
        "    if z.imag<=0:\n",
        "      sqz=-sqz\n",
        "    f_S.append([sqz.real,(sqz.imag)**2,m_f])\n",
        "  return f_S\n",
        "\n",
        "\n",
        "#MCMC and observables functions\n",
        "\n",
        "def serial_sample_generator(model, action, L, beta, batch_size, N_samples): \n",
        "    layers, prior_U, prior_x = model['layers'], model['prior_U'], model['prior_x']\n",
        "    layers.eval()\n",
        "    U, x, E, logq, logp = None, None, None, None, None\n",
        "    for i in range(N_samples):\n",
        "        batch_i = i % batch_size\n",
        "        if batch_i == 0:\n",
        "            # we're out of samples to propose, generate a new batch\n",
        "            with torch.no_grad():\n",
        "              U, x, logq = apply_flow_to_prior(prior_U, prior_x, layers, batch_size=batch_size) \n",
        "              logp, E = action(U,x)\n",
        "              logp=-logp\n",
        "              E=-E/(beta*L*L)\n",
        "              Q=topo_charge(U)\n",
        "        yield E[batch_i], logq[batch_i], logp[batch_i], Q[batch_i]\n",
        "\n",
        "def sample_generator_new(beta, prior_U, prior_x, layers, batch_size): \n",
        "  with torch.no_grad():\n",
        "    U, x, logq = apply_flow_to_prior(prior_U, prior_x, layers, batch_size=batch_size) \n",
        "    logp, E = action(U,x)\n",
        "    logp=-logp\n",
        "    E=-E/(beta*L*L)\n",
        "    Q=topo_charge(U)\n",
        "  return grab(logp), grab(logq), grab(E), grab(Q)\n",
        "\n",
        "def make_mcmc_ensemble(model, action, L, beta, batch_size, N_samples):\n",
        "    history = {\n",
        "        'E' : [], \n",
        "        'logq' : [], \n",
        "        'logp' : [], \n",
        "        'accepted' : [],\n",
        "        'Q' : []\n",
        "\n",
        "    }  \n",
        "    # build Markov chain\n",
        "    sample_gen = serial_sample_generator(model, action, L, beta, batch_size, N_samples)\n",
        "    for new_E, new_logq, new_logp, new_Q in sample_gen:\n",
        "        if len(history['logp']) == 0:\n",
        "            # always accept first proposal, Markov chain must start somewhere \n",
        "            accepted = True\n",
        "        else:\n",
        "            # Metropolis acceptance condition\n",
        "            last_logp = history['logp'][-1]\n",
        "            last_logq = history['logq'][-1]\n",
        "            p_accept = torch.exp((new_logp - new_logq) - (last_logp - last_logq)) \n",
        "            p_accept = min(1, p_accept)\n",
        "            draw = torch.rand(1) # ~ [0,1]\n",
        "            if draw < p_accept:\n",
        "                accepted = True\n",
        "            else:\n",
        "                accepted = False\n",
        "                new_E = history['E'][-1] \n",
        "                new_Q = history['Q'][-1] \n",
        "                new_logp = last_logp \n",
        "                new_logq = last_logq\n",
        "        # Update Markov chain\n",
        "        history['logp'].append(new_logp) \n",
        "        history['logq'].append(new_logq) \n",
        "        history['E'].append(new_E) \n",
        "        history['accepted'].append(accepted)\n",
        "        history['Q'].append(new_Q)\n",
        "    return history\n",
        "\n",
        "\n",
        "def make_mcmc_ensemble_new(model, action, L, beta, batch_size, N_batches):\n",
        "  history = {\n",
        "    'E' : [], \n",
        "    'logq' : [], \n",
        "    'logp' : [], \n",
        "    'accepted' : [],\n",
        "    'Q' : []\n",
        "  }  \n",
        "  layers, prior_U, prior_x = model['layers'], model['prior_U'], model['prior_x']\n",
        "  layers.eval()\n",
        "\n",
        "  # build Markov chain\n",
        "  for i in range(N_batches):\n",
        "    logp, logq, E, Q=sample_generator_new(beta, prior_U, prior_x, layers, batch_size=batch_size)\n",
        "    for j in range(batch_size):\n",
        "      if len(history['logp']) == 0:\n",
        "        # always accept first proposal, Markov chain must start somewhere \n",
        "        accepted = True\n",
        "        new_E=E[j]\n",
        "        new_Q=Q[j]\n",
        "        new_logp=logp[j]\n",
        "        new_logq=logq[j]\n",
        "      else:\n",
        "        # Metropolis acceptance condition\n",
        "        last_logp = history['logp'][-1]\n",
        "        last_logq = history['logq'][-1]\n",
        "        p_accept = np.exp((logp[j] - logq[j]) - (last_logp - last_logq)) \n",
        "        p_accept = min(1, p_accept)\n",
        "        draw = np.random.rand(1) # ~ [0,1]\n",
        "        if draw < p_accept:\n",
        "          accepted = True\n",
        "          new_E=E[j]\n",
        "          new_Q=Q[j]\n",
        "          new_logp=logp[j]\n",
        "          new_logq=logq[j]\n",
        "        else:\n",
        "          accepted = False\n",
        "          new_E = history['E'][-1] \n",
        "          new_Q = history['Q'][-1] \n",
        "          new_logp = last_logp \n",
        "          new_logq = last_logq\n",
        "      # Update Markov chain\n",
        "      history['logp'].append(new_logp) \n",
        "      history['logq'].append(new_logq) \n",
        "      history['E'].append(new_E) \n",
        "      history['Q'].append(new_Q)\n",
        "      history['accepted'].append(accepted)\n",
        "  return history\n",
        "\n",
        "def bootstrap(x, *, Nboot, binsize):\n",
        "    boots = []\n",
        "    x = x.reshape(-1, binsize, *x.shape[1:])\n",
        "    for i in range(Nboot):\n",
        "        boots.append(np.mean(x[np.random.randint(len(x), size=len(x))], axis=(0,1)))\n",
        "    \n",
        "    plt.figure(figsize=(5,3.5), dpi=125)\n",
        "    plt.hist(boots,bins=100)\n",
        "    plt.show()\n",
        "    return np.mean(boots), np.std(boots)\n",
        "\n",
        "def topo_charge(x):\n",
        "    P01 = torch_wrap(compute_u1_plaq(x, mu=0, nu=1))\n",
        "    axes = tuple(range(1, len(P01.shape)))\n",
        "    return torch.sum(P01, dim=axes) / (2*np.pi)\n",
        "\n",
        "\n",
        "#Actual training\n",
        "\n",
        "# Theory\n",
        "\n",
        "# Action\n",
        "L=8\n",
        "n_f=10\n",
        "lattice_shape = (L,L)\n",
        "link_shape = (2,L,L)\n",
        "x_shape = (n_f,L,L,2)\n",
        "beta = 2.0\n",
        "f_S=naivepolygen(n_f,-0.11)\n",
        "#Neural net hyperparameters\n",
        "n_layers_g = 24\n",
        "n_layers_f = 16\n",
        "n_s_nets = 2\n",
        "#L16n8m-011- runs with no prefix\n",
        "hidden_sizes = [[32,32],[32,32],[32,32]]\n",
        "dilation_sizes= [[1,2,3],[1,1,1],[1,1,1]]\n",
        "#L16n8m-011- runs with prefix [24,24,24,24] fermion\n",
        "#hidden_sizes = [[32,32],[24,24,24,24],[24,24,24,24]]\n",
        "#dilation_sizes= [[1,2,3],[1,1,1,1,1],[1,1,1,1,1]]\n",
        "#L16n8m-011-nodil runs\n",
        "#hidden_sizes = [[32,32,32],[32,32,32],[32,32,32]]\n",
        "#dilation_sizes= [[1,1,1,1],[1,1,1,1],[1,1,1,1]]\n",
        "kernel_size = 3\n",
        "base_lr = .001\n",
        "\n",
        "\n",
        "# Model\n",
        "prior_U = MultivariateUniform((-1)*np.pi*torch.ones(link_shape), np.pi*torch.ones(link_shape))\n",
        "prior_x = Fermion_nf(torch.zeros(x_shape),0.7*torch.ones(x_shape),torch.zeros(x_shape),0.7*torch.ones(x_shape))\n",
        "#prior_test = Fermion_nf((-1)*np.pi*torch.ones(x_shape), np.pi*torch.ones(x_shape),torch.zeros(x_shape),0.1*torch.ones(x_shape))\n",
        "action=TotalAction(beta,f_S,n_f)\n",
        "#action=U1GaugeAction(beta)\n",
        "#action=TestAction(prior_U,prior_test)\n",
        "\n",
        "layers = make_phi4u1_equiv_layers(lattice_shape=lattice_shape, x_shape=x_shape,n_layers_g=n_layers_g, n_layers_f=n_layers_f,n_mixture_comps=n_s_nets,\n",
        "                            hidden_sizes=hidden_sizes, dilation_sizes=dilation_sizes, kernel_size=kernel_size, n_f=n_f,f_S=f_S) \n",
        "#set_weights(layers)\n",
        "set_weights_xavier(layers)\n",
        "model = {'layers': layers, 'prior_U': prior_U, 'prior_x': prior_x}\n",
        "optimizer = torch.optim.AdamW(model['layers'].parameters(), lr=base_lr,eps=1e-02)\n",
        "\n",
        "# Training\n",
        "LoadPATH=\"L8n10m-011-900batch-12520step-l10-3_10-4-l10-5\"\n",
        "SavePATH=\"temp\"\n",
        "use_pretrained = True\n",
        "continue_training = False\n",
        "\n",
        "if continue_training:\n",
        "  print('Loading model for further training')\n",
        "  model['layers'].load_state_dict(torch.load(LoadPATH)) \n",
        "  if torch_device == 'cuda':\n",
        "    model['layers'].cuda() \n",
        "\n",
        "if use_pretrained:\n",
        "  print('Loading pre-trained model')\n",
        "  model['layers'].load_state_dict(torch.load(LoadPATH)) \n",
        "  if torch_device == 'cuda':\n",
        "    model['layers'].cuda() \n",
        "else:\n",
        "  print('Skipping pre-trained model')\n",
        "\n",
        "\n",
        "\n",
        "N_era = 1100\n",
        "N_epoch = 10\n",
        "batch_size = 12000\n",
        "print_freq = N_epoch # epochs\n",
        "plot_freq = 1 # epochs\n",
        "history = {\n",
        "    'loss' : [],\n",
        "    'logp' : [],\n",
        "    'logq' : [],\n",
        "    'ess' : [],\n",
        "    'E' : [],\n",
        "    'x' : []\n",
        "}\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[500,1000], gamma=0.1)\n",
        "[plt.close(plt.figure(fignum)) for fignum in plt.get_fignums()] # close all existing figures \n",
        "if not use_pretrained:\n",
        "  live_plot = init_live_plot()\n",
        "  for era in range(N_era):\n",
        "    for epoch in range(N_epoch):\n",
        "      train_step(model, action, calc_dkl, optimizer, history)\n",
        "      if epoch % print_freq == 0:\n",
        "        print_metrics(history, avg_last_N_epochs=print_freq)\n",
        "      if epoch % plot_freq == 0:\n",
        "        update_plots(history, **live_plot)\n",
        "    scheduler.step()\n",
        "      \n",
        "  torch.save(model['layers'].state_dict(), SavePATH) \n",
        "else:\n",
        "  print('Skipping training')\n",
        "\n",
        "\n",
        "#Evaluation\n",
        "batch_size=batch_size\n",
        "N_batches=300\n",
        "tot_ens = make_mcmc_ensemble_new(model, action, L, beta, batch_size, N_batches)\n",
        "print(\"Accept rate:\", np.mean(tot_ens['accepted']))\n",
        "n_therm = 50*batch_size\n",
        "E_cnfg = np.array(tot_ens['E'])[n_therm:]\n",
        "A_cnfg= np.array(tot_ens['accepted'])[n_therm:]\n",
        "A_mean, A_err=bootstrap(A_cnfg, Nboot=10000, binsize=600)\n",
        "Q = np.array(tot_ens['Q'])[n_therm:]\n",
        "print(f'Accept rate={A_mean:} +/- {A_err:}')\n",
        "E_mean, E_err = bootstrap(E_cnfg, Nboot=10000, binsize=600)\n",
        "print(f'average plaquette = {E_mean:} +/- {E_err:}') \n",
        "X_mean, X_err = bootstrap(Q**2, Nboot=10000, binsize=600) \n",
        "print(f'Topological susceptibility = {X_mean:} +/- {X_err:}')\n",
        "\n",
        "plt.figure(figsize=(5,3.5), dpi=125)\n",
        "plt.plot(E_cnfg)\n",
        "plt.xlabel(r'$t_{MC}$')\n",
        "plt.ylabel(r'average plaquette $P$') \n",
        "plt.show()\n",
        "plt.figure(figsize=(5,3.5), dpi=125)\n",
        "plt.plot(Q)\n",
        "plt.xlabel(r'$t_{MC}$')\n",
        "plt.ylabel(r'topological charge $Q$') \n",
        "plt.show()\n",
        "plt.figure(figsize=(5,3.5), dpi=125)\n",
        "plt.plot(Q**2)\n",
        "plt.xlabel(r'$t_{MC}$')\n",
        "plt.ylabel(r'topological susceptibility $Q**2$') \n",
        "plt.show()\n",
        "plt.figure(figsize=(5,3.5), dpi=125)\n",
        "plt.plot(A_cnfg)\n",
        "plt.xlabel(r'$t_{MC}$')\n",
        "plt.ylabel(r'acceptance') \n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SVHWB93OXcWI"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLlOgFBwH+SDNYlcKbA8zn",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}